{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, we load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "path_df = \"/Users/tissatechnolgy/Desktop/UNTProject/ICE-1/Codes/Pickles/blog_pickle_files/blog_pickles/df.pickle\"\n",
    "with open(path_df, 'rb') as data:\n",
    "    df = pickle.load(data)\n",
    "\n",
    "# features_train\n",
    "path_features_train = \"/Users/tissatechnolgy/Desktop/UNTProject/ICE-1/Codes/Pickles/blog_pickle_files/blog_pickles/features_train.pickle\"\n",
    "with open(path_features_train, 'rb') as data:\n",
    "    features_train = pickle.load(data)\n",
    "\n",
    "# labels_train\n",
    "path_labels_train = \"/Users/tissatechnolgy/Desktop/UNTProject/ICE-1/Codes/Pickles/blog_pickle_files/blog_pickles/labels_train.pickle\"\n",
    "with open(path_labels_train, 'rb') as data:\n",
    "    labels_train = pickle.load(data)\n",
    "\n",
    "# features_test\n",
    "path_features_test = \"/Users/tissatechnolgy/Desktop/UNTProject/ICE-1/Codes/Pickles/blog_pickle_files/blog_pickles/features_test.pickle\"\n",
    "with open(path_features_test, 'rb') as data:\n",
    "    features_test = pickle.load(data)\n",
    "\n",
    "# labels_test\n",
    "path_labels_test = \"/Users/tissatechnolgy/Desktop/UNTProject/ICE-1/Codes/Pickles/blog_pickle_files/blog_pickles/labels_test.pickle\"\n",
    "with open(path_labels_test, 'rb') as data:\n",
    "    labels_test = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's check the dimension of our feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1891, 300)\n",
      "(334, 300)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation for Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 8,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "rf_0 = RandomForestClassifier(random_state = 8)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf_0.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Randomized Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [20, 40, 60, 80, 100, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000]}\n"
     ]
    }
   ],
   "source": [
    "# n_estimators\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "\n",
    "# max_features\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# max_depth\n",
    "max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# min_samples_split\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# min_samples_leaf\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# bootstrap\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(random_state=8),\n",
       "                   n_iter=50,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [20, 40, 60, 80, 100,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000]},\n",
       "                   random_state=8, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create the base model to tune\n",
    "rfc = RandomForestClassifier(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=rfc,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'bootstrap': False}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9434181068095491\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "bootstrap = [False]\n",
    "max_depth = [30, 40, 50]\n",
    "max_features = ['sqrt']\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "min_samples_split = [5, 10, 15]\n",
    "n_estimators = [800]\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': bootstrap,\n",
    "    'max_depth': max_depth,\n",
    "    'max_features': max_features,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'n_estimators': n_estimators\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rfc = RandomForestClassifier(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=rfc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfc = grid_search.best_estimator_\n",
    "best_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfc.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pred = best_rfc.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(labels_train, best_rfc.predict(features_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(labels_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification report\")\n",
    "print(classification_report(labels_test,rfc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_df = df[['Category', 'Category_Code']].drop_duplicates().sort_values('Category_Code')\n",
    "conf_matrix = confusion_matrix(labels_test, rfc_pred)\n",
    "plt.figure(figsize=(12.8,6))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            xticklabels=aux_df['Category'].values, \n",
    "            yticklabels=aux_df['Category'].values,\n",
    "            cmap=\"Blues\")\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At this point, we could get the average time the model takes to get predictions. We want the algorithm to be fast since we are creating an app which will gather data from the internet and get the predicted categories. However, since the difference when predicting 10-20 observations will be very little, we won't take this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "features_time = features_train\n",
    "elapsed_list = []\n",
    "for i in range(0,10):\n",
    "\n",
    "    start = time.time()\n",
    "    predictions = best_rfc.predict(features_time)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    elapsed_list.append(elapsed)\n",
    "\n",
    "mean_time_elapsed = np.mean(elapsed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = RandomForestClassifier(random_state = 8)\n",
    "base_model.fit(features_train, labels_train)\n",
    "accuracy_score(labels_test, base_model.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfc.fit(features_train, labels_train)\n",
    "accuracy_score(labels_test, best_rfc.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "     'Model': 'Random Forest',\n",
    "     'Training Set Accuracy': accuracy_score(labels_train, best_rfc.predict(features_train)),\n",
    "     'Test Set Accuracy': accuracy_score(labels_test, rfc_pred)\n",
    "}\n",
    "\n",
    "df_models_rfc = pd.DataFrame(d, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's save the model and this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Models/best_rfc.pickle', 'wb') as output:\n",
    "    pickle.dump(best_rfc, output)\n",
    "    \n",
    "with open('Models/df_models_rfc.pickle', 'wb') as output:\n",
    "    pickle.dump(df_models_rfc, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
